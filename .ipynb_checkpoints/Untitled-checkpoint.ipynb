{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'api_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-aea8eb64bcc5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# in from another file source, shown below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'/Users/symbioticpenguin/Documents'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mapi_keys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcomdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtomdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiania_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiania_OMDB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjomdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'api_keys'"
     ]
    }
   ],
   "source": [
    "#IMPORT DEPENDENCIES\n",
    "import sys, requests\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Read the Netflix data as a pandas dataframe\n",
    "data_table = \"./Resources/netflix_titles.csv\"\n",
    "netflix_df = pd.read_csv(data_table)\n",
    "\n",
    "\n",
    "# The user will have to use their own API key when running this code; I pulled our API keys\n",
    "# in from another file source, shown below\n",
    "sys.path.insert(1, '/Users/symbioticpenguin/Documents')\n",
    "from api_keys import comdb, tomdb, tiania_api, tiania_OMDB, jomdb\n",
    "\n",
    "\n",
    "# Filtering for movies only\n",
    "filtered_flix = netflix_df[netflix_df[\"type\"]==\"Movie\"]\n",
    "\n",
    "# Filtering for only movies made in the United States\n",
    "filtered_flix = filtered_flix[netflix_df[\"country\"] == \"United States\"]\n",
    "\n",
    "# Dropping all NaN rows\n",
    "filtered_flix = filtered_flix.dropna()\n",
    "\n",
    "# limiting to 1000 entries because free api calls only allow 1000 :(\n",
    "filtered_flix = filtered_flix[:1000]\n",
    "\n",
    "# LeTs TaKe A lOoK aT tHe DaTa\n",
    "filtered_flix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the test code we used before calling the API 1,000 times!\n",
    "\n",
    "# movie = \"t=\" + \"Finding Nemo\"\n",
    "# key = \"&apikey=\"+comdb\n",
    "# url = 'http://www.omdbapi.com/?' + movie +  key\n",
    "# response = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains the for loop to get all the data for movies in netflix in the omdb api\n",
    "\n",
    "# Creating the list of movie titles to make the API calls on\n",
    "filtered_movies = filtered_flix['title']\n",
    "\n",
    "# initializing empty lists to append to in the loop\n",
    "metascore = []\n",
    "imdbrating = []\n",
    "imdbvotes = []\n",
    "boxoffice = []\n",
    "title = []\n",
    "\n",
    "# This is the api call for our OMDB dataframe\n",
    "for item in filtered_movies:\n",
    "    \n",
    "    # If the loop doesn't work, we will skip the API call and list append function.\n",
    "    try:\n",
    "        # Creating the string for the URL to call the API from.\n",
    "        movie = \"t=\" + item\n",
    "        key = \"&apikey=\"+jomdb\n",
    "        url = 'http://www.omdbapi.com/?' + movie +  key\n",
    "        \n",
    "        # Storing the response as a json object\n",
    "        response = requests.get(url).json()\n",
    "        \n",
    "        # Append the items we care about from the json\n",
    "        boxoffice.append(response['BoxOffice'])\n",
    "        title.append(response['Title'])\n",
    "        metascore.append(response[\"Metascore\"])\n",
    "        imdbrating.append(response['imdbRating'])\n",
    "        imdbvotes.append(response['imdbVotes'])\n",
    "\n",
    "    except:\n",
    "        next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the DataFrame for the OMDB data\n",
    "omdb_df = pd.DataFrame({'title':title,\n",
    "                        'metascore':metascore,\n",
    "                        'imdb_rating':imdbrating,\n",
    "                        'imdb_votes':imdbvotes,\n",
    "                        'box_office':boxoffice})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the two tables: the netflix table and the OMDB table\n",
    "final_table = filtered_flix.join(omdb_df.set_index('title'),on = 'title', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing away with the rows that contain \"NaN\" in any column.\n",
    "final_table = final_table.dropna()\n",
    "\n",
    "# Write final table to csv for use later\n",
    "final_table.to_csv('final_table.csv',index = False, header = True, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to local data base\n",
    "rds_connection_string = \"postgres:postgres@localhost:5432/ETLmovies_db\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for tables \n",
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload data frame to database\n",
    "final_table.to_sql(name='final_table_db', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dataframe made it to the table\n",
    "pd.read_sql_query('select * from final_table_db', con=engine).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
